---
# Namespace for monitoring
apiVersion: v1
kind: Namespace
metadata:
  name: monitoring
---
# Prometheus ConfigMap (for low resource usage, collects from all namespaces)
apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus-server-conf
  namespace: monitoring
data:
  prometheus.yml: |
    global:
      scrape_interval: 2s  # Collect metrics every 2 seconds

    # Alertmanager config
    alerting:
      alertmanagers:
        - static_configs:
            - targets:
                - alertmanager.monitoring.svc.cluster.local:9093

    rule_files:
      - "/etc/prometheus/rules/*.yaml"  # Alert rules

    scrape_configs:
      - job_name: 'kubernetes-pods'
        kubernetes_sd_configs:
          - role: pod
        relabel_configs:
          - action: keep
            source_labels: [__meta_kubernetes_namespace]
            regex: .*

      - job_name: 'kubernetes-nodes'
        kubernetes_sd_configs:
          - role: node

      - job_name: 'kubernetes-service-endpoints'
        kubernetes_sd_configs:
          - role: endpoints

      - job_name: 'kubernetes-cadvisor'
        kubernetes_sd_configs:
          - role: node
        metrics_path: /metrics/cadvisor

      - job_name: 'kubernetes-apiserver'
        kubernetes_sd_configs:
          - role: endpoints
        scheme: https
        tls_config:
          ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
          insecure_skip_verify: true
        bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
        relabel_configs:
          - action: keep
            source_labels: [__meta_kubernetes_namespace]
            regex: "kube-system"

      - job_name: 'kubelet'
        metrics_path: /metrics
        scheme: https
        kubernetes_sd_configs:
          - role: node
        tls_config:
          ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
          insecure_skip_verify: true
        bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token

      - job_name: 'kube-state-metrics'
        static_configs:
          - targets: ['kube-state-metrics.monitoring.svc.cluster.local:8080']

---
# Prometheus Deployment (with resource limits, using latest stable image)
apiVersion: apps/v1
kind: Deployment
metadata:
  name: prometheus-server
  namespace: monitoring
spec:
  replicas: 1
  selector:
    matchLabels:
      app: prometheus-server
  template:
    metadata:
      labels:
        app: prometheus-server
    spec:
      containers:
      - name: prometheus
        image: prom/prometheus:v2.47.0  # Specific Prometheus version
        args:
          - "--config.file=/etc/prometheus/prometheus.yml"
          - "--storage.tsdb.path=/prometheus/"
          - "--storage.tsdb.retention.time=7d"  # Retain data for 7 days
        ports:
        - containerPort: 9090
        volumeMounts:
        - name: prometheus-storage
          mountPath: /prometheus
        - name: prometheus-config
          mountPath: /etc/prometheus/
        resources:
          requests:
            memory: "256Mi"
            cpu: "200m"
          limits:
            memory: "512Mi"
            cpu: "400m"
      volumes:
      - name: prometheus-storage
        emptyDir: {}  # For production, consider PersistentVolumeClaim
      - name: prometheus-config
        configMap:
          name: prometheus-server-conf
---
# Prometheus Service
apiVersion: v1
kind: Service
metadata:
  name: prometheus-server
  namespace: monitoring
spec:
  type: NodePort
  ports:
    - port: 9090
      targetPort: 9090
      nodePort: 30900
  selector:
    app: prometheus-server
---
# Prometheus Alertmanager Deployment (using stable version)
apiVersion: apps/v1
kind: Deployment
metadata:
  name: alertmanager
  namespace: monitoring
spec:
  replicas: 1
  selector:
    matchLabels:
      app: alertmanager
  template:
    metadata:
      labels:
        app: alertmanager
    spec:
      containers:
      - name: alertmanager
        image: prom/alertmanager:v0.25.0  # Pin to specific stable version
        imagePullPolicy: IfNotPresent
        args:
          - "--config.file=/etc/alertmanager/alertmanager.yml"
          - "--storage.path=/alertmanager"
        ports:
        - containerPort: 9093
        volumeMounts:
        - name: alertmanager-config
          mountPath: /etc/alertmanager/
        - name: alertmanager-storage
          mountPath: /alertmanager
      volumes:
      - name: alertmanager-storage
        emptyDir: {}  # Consider using PersistentVolumeClaim for production
      - name: alertmanager-config
        configMap:
          name: alertmanager-config
---
# Alertmanager ConfigMap
apiVersion: v1
kind: ConfigMap
metadata:
  name: alertmanager-config
  namespace: monitoring
data:
  alertmanager.yml: |
    global:
      resolve_timeout: 1m
      slack_api_url: "https://hooks.slack.com/services/XXXXXXXXXXX"  # Replace with your actual Slack webhook URL

    route:
      receiver: 'slack-notifications'
      group_by: ['alertname']
      group_wait: 30s
      group_interval: 5m
      repeat_interval: 3h

    receivers:
      - name: 'slack-notifications'
        slack_configs:
          - channel: '#monitoring-alert'  # Replace with your Slack channel name
            send_resolved: true
            icon_url: "https://avatars3.githubusercontent.com/u/3380462"  # Correct URL format
            title: "[{{ .Status | toUpper }}] {{ .CommonLabels.alertname }}"
            text: >-
              *Alert:* {{ .Annotations.title }}
              *Description:* {{ .Annotations.description }}
              *Severity:* {{ .Labels.severity }}

    inhibit_rules:
      - source_match:
          severity: 'critical'
        target_match:
          severity: 'warning'
        equal: ['alertname', 'job', 'instance']


---
# Alertmanager Service
apiVersion: v1
kind: Service
metadata:
  name: alertmanager
  namespace: monitoring
spec:
  type: NodePort
  ports:
    - port: 9093
      targetPort: 9093
      nodePort: 30903
  selector:
    app: alertmanager
---
# Grafana Deployment (pinned to stable version)
apiVersion: apps/v1
kind: Deployment
metadata:
  name: grafana
  namespace: monitoring
spec:
  replicas: 1
  selector:
    matchLabels:
      app: grafana
  template:
    metadata:
      labels:
        app: grafana
    spec:
      containers:
      - name: grafana
        image: grafana/grafana:11.1.7 # Pinned Grafana version
        ports:
        - containerPort: 3000
        env:
        - name: GF_SECURITY_ADMIN_PASSWORD
          value: "admin"  # Change this password for production
        resources:
          requests:
            memory: "64Mi"
            cpu: "50m"
          limits:
            memory: "128Mi"
            cpu: "100m"
---
# Grafana Service
apiVersion: v1
kind: Service
metadata:
  name: grafana
  namespace: monitoring
spec:
  type: NodePort
  ports:
    - port: 3000
      targetPort: 3000
      nodePort: 30001
  selector:
    app: grafana
---
# Grafana ConfigMap (for Prometheus data source configuration)
apiVersion: v1
kind: ConfigMap
metadata:
  name: grafana-datasources
  namespace: monitoring
data:
  datasources.yaml: |
    apiVersion: 1
    datasources:
    - name: Prometheus
      type: prometheus
      url: http://prometheus-server.monitoring.svc.cluster.local:9090
      access: proxy
      isDefault: true
---
# kube-state-metrics Deployment (pinned version)
apiVersion: apps/v1
kind: Deployment
metadata:
  name: kube-state-metrics
  namespace: monitoring
spec:
  replicas: 1
  selector:
    matchLabels:
      app: kube-state-metrics
  template:
    metadata:
      labels:
        app: kube-state-metrics
    spec:
      containers:
      - name: kube-state-metrics
        image: bitnami/kube-state-metrics:2.7.0  # Pinned version
        ports:
        - containerPort: 8080
        resources:
          requests:
            memory: "64Mi"
            cpu: "50m"
          limits:
            memory: "128Mi"
            cpu: "100m"
---
# kube-state-metrics Service
apiVersion: v1
kind: Service
metadata:
  name: kube-state-metrics
  namespace: monitoring
spec:
  ports:
    - name: http
      port: 8080
      targetPort: 8080
  selector:
    app: kube-state-metrics
---
# Node Exporter DaemonSet (pinned version)
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: node-exporter
  namespace: monitoring
spec:
  selector:
    matchLabels:
      app: node-exporter
  template:
    metadata:
      labels:
        app: node-exporter
    spec:
      containers:
      - name: node-exporter
        image: prom/node-exporter:v1.7.0  # Pinned node-exporter version
        ports:
        - containerPort: 9100
        resources:
          requests:
            memory: "64Mi"
            cpu: "50m"
          limits:
            memory: "128Mi"
            cpu: "100m"
        securityContext:
          privileged: true
---
# Node Exporter Service
apiVersion: v1
kind: Service
metadata:
  name: node-exporter
  namespace: monitoring
spec:
  ports:
    - name: http
      port: 9100
      targetPort: 9100
  selector:
    app: node-exporter
